# Classical CSSR Analysis Framework
## Comprehensive Evaluation System for Unified Datasets

### Overview
This framework provides comprehensive analysis of classical CSSR performance on datasets generated by the unified dataset generation system. It leverages the rich ground truth metadata and statistical analysis to provide rigorous evaluation and baseline establishment.

## Framework Architecture

### Core Components
```
ClassicalCSSRAnalyzer
├── Dataset Loader        # Load unified dataset formats
├── CSSR Runner          # Execute classical CSSR algorithm  
├── Ground Truth Evaluator # Compare against known structures
├── Performance Analyzer  # Comprehensive metrics computation
├── Results Visualizer   # Analysis visualization and reporting
└── Baseline Establisher # Create benchmark metrics
```

### Integration with Unified Datasets
```
datasets/experiment_name/
├── raw_sequences/           ← Load sequences for CSSR
├── ground_truth/           ← Load true machine definitions
├── statistical_analysis/   ← Load theoretical baselines
└── quality_reports/        ← Load dataset quality info
        ↓
ClassicalCSSRAnalyzer
        ↓
results/classical_analysis/
├── cssr_results.json      # Discovered structures
├── evaluation_metrics.json # Performance against ground truth
├── baseline_comparison.json # vs theoretical optima
└── analysis_report.html   # Comprehensive report
```

## Implementation Specification

### 1. Main Analysis Class (`classical_analyzer.py`)

```python
class ClassicalCSSRAnalyzer:
    """
    Comprehensive classical CSSR analysis for unified datasets.
    
    Provides end-to-end analysis pipeline from dataset loading
    to comprehensive evaluation against ground truth.
    """
    
    def __init__(self, dataset_dir: str, output_dir: str):
        """
        Initialize analyzer with dataset and output directories.
        
        Args:
            dataset_dir: Path to unified dataset directory
            output_dir: Path for analysis results
        """
        self.dataset_dir = Path(dataset_dir)
        self.output_dir = Path(output_dir)
        self.dataset_metadata = None
        self.ground_truth = None
        self.cssr_results = None
        
    def run_complete_analysis(self, max_length: int = 10, 
                            significance_level: float = 0.05) -> Dict[str, Any]:
        """
        Run complete classical CSSR analysis pipeline.
        
        Returns:
            comprehensive_results: All analysis results and metrics
        """
        # Step 1: Load dataset and metadata
        self.load_dataset()
        
        # Step 2: Run classical CSSR
        self.run_classical_cssr(max_length, significance_level)
        
        # Step 3: Evaluate against ground truth
        evaluation_metrics = self.evaluate_against_ground_truth()
        
        # Step 4: Compute performance baselines
        baseline_comparison = self.compare_against_baselines()
        
        # Step 5: Analyze scaling behavior
        scaling_analysis = self.analyze_scaling_behavior()
        
        # Step 6: Generate comprehensive report
        results = self.generate_analysis_report(
            evaluation_metrics, baseline_comparison, scaling_analysis
        )
        
        return results
```

### 2. Dataset Integration (`dataset_loader.py`)

```python
class UnifiedDatasetLoader:
    """Load and parse unified dataset formats for classical CSSR."""
    
    def load_sequences(self, dataset_dir: str, split: str = 'train') -> List[str]:
        """
        Load raw sequences in classical CSSR format.
        
        Args:
            dataset_dir: Unified dataset directory
            split: 'train', 'val', or 'test'
            
        Returns:
            List of sequence strings
        """
        seq_file = Path(dataset_dir) / 'raw_sequences' / f'{split}_sequences.txt'
        with open(seq_file, 'r') as f:
            return [line.strip() for line in f.readlines()]
    
    def load_ground_truth(self, dataset_dir: str) -> Dict[str, Any]:
        """
        Load ground truth machine definitions and causal states.
        
        Returns:
            {
                'machines': {...},           # Machine definitions
                'causal_state_labels': {...}, # True state sequences
                'optimal_predictions': {...}  # Theoretical optima
            }
        """
        gt_dir = Path(dataset_dir) / 'ground_truth'
        
        ground_truth = {}
        for file_name in ['machine_definitions.json', 'causal_state_labels.json', 
                         'transition_matrices.json', 'optimal_predictions.json']:
            file_path = gt_dir / file_name
            if file_path.exists():
                with open(file_path, 'r') as f:
                    key = file_name.replace('.json', '')
                    ground_truth[key] = json.load(f)
        
        return ground_truth
    
    def load_statistical_metadata(self, dataset_dir: str) -> Dict[str, Any]:
        """Load comprehensive statistical analysis metadata."""
        stats_dir = Path(dataset_dir) / 'statistical_analysis'
        
        metadata = {}
        for file_name in ['information_measures.json', 'complexity_metrics.json',
                         'sequence_statistics.json', 'comparative_baselines.json']:
            file_path = stats_dir / file_name
            if file_path.exists():
                with open(file_path, 'r') as f:
                    key = file_name.replace('.json', '')
                    metadata[key] = json.load(f)
        
        return metadata
```

### 3. CSSR Execution Engine (`cssr_runner.py`)

```python
class CSSRExecutionEngine:
    """Execute classical CSSR with comprehensive parameter exploration."""
    
    def __init__(self, sequences: List[str]):
        self.sequences = sequences
        self.results = {}
        
    def run_parameter_sweep(self, max_lengths: List[int] = [6, 8, 10, 12],
                          significance_levels: List[float] = [0.001, 0.01, 0.05]) -> Dict[str, Any]:
        """
        Run CSSR across multiple parameter combinations.
        
        Returns:
            {
                'parameter_results': {...},  # Results for each (L, α) pair
                'best_parameters': {...},    # Optimal parameter selection
                'convergence_analysis': {...} # Convergence behavior
            }
        """
        results = {}
        
        for max_length in max_lengths:
            for significance_level in significance_levels:
                param_key = f"L{max_length}_alpha{significance_level}"
                
                print(f"Running CSSR with L={max_length}, α={significance_level}")
                
                # Run classical CSSR
                cssr = ClassicalCSSR(self.sequences)
                converged = cssr.run_cssr(
                    max_iterations=20,
                    max_history_length=max_length,
                    significance_level=significance_level
                )
                
                # Extract results
                results[param_key] = {
                    'parameters': {
                        'max_length': max_length,
                        'significance_level': significance_level
                    },
                    'converged': converged,
                    'num_states': len(cssr.causal_states),
                    'states': self._extract_state_info(cssr.causal_states),
                    'transitions': self._extract_transition_info(cssr.causal_states),
                    'computational_stats': {
                        'total_histories': len(cssr.history_counts),
                        'runtime_seconds': cssr.runtime if hasattr(cssr, 'runtime') else None
                    }
                }
        
        return {
            'parameter_results': results,
            'best_parameters': self._select_best_parameters(results),
            'convergence_analysis': self._analyze_convergence(results)
        }
    
    def _extract_state_info(self, causal_states: List) -> Dict[str, Any]:
        """Extract detailed information about discovered states."""
        states_info = {}
        
        for state in causal_states:
            state_info = {
                'histories': list(state.histories),
                'history_count': len(state.histories),
                'total_observations': state.total_count,
                'symbol_distribution': dict(state.next_symbol_distribution),
                'entropy': self._compute_state_entropy(state.next_symbol_distribution)
            }
            states_info[f"State_{state.state_id}"] = state_info
            
        return states_info
    
    def _compute_state_entropy(self, symbol_dist: Dict[str, int]) -> float:
        """Compute entropy of state's emission distribution."""
        total = sum(symbol_dist.values())
        if total == 0:
            return 0.0
            
        entropy = 0.0
        for count in symbol_dist.values():
            if count > 0:
                prob = count / total
                entropy -= prob * np.log2(prob)
        
        return entropy
```

### 4. Ground Truth Evaluation (`evaluation_engine.py`)

```python
class GroundTruthEvaluator:
    """Comprehensive evaluation against known ground truth."""
    
    def __init__(self, cssr_results: Dict, ground_truth: Dict):
        self.cssr_results = cssr_results
        self.ground_truth = ground_truth
        
    def evaluate_structure_recovery(self) -> Dict[str, float]:
        """
        Evaluate how well CSSR recovered the true causal structure.
        
        Returns:
            {
                'state_count_accuracy': float,      # |discovered| vs |true|
                'transition_recovery_rate': float,  # Fraction of true transitions found
                'spurious_transition_rate': float,  # Fraction of false transitions
                'history_assignment_accuracy': float, # Correct history→state mapping
                'information_distance': float       # Information-theoretic distance
            }
        """
        metrics = {}
        
        # State count comparison
        true_machines = self.ground_truth.get('machine_definitions', {})
        if true_machines:
            true_state_counts = [len(m['states']) for m in true_machines.values()]
            avg_true_states = np.mean(true_state_counts)
            
            discovered_states = [r['num_states'] for r in self.cssr_results['parameter_results'].values()]
            avg_discovered_states = np.mean(discovered_states)
            
            metrics['state_count_accuracy'] = 1.0 - abs(avg_discovered_states - avg_true_states) / avg_true_states
        
        # Transition recovery analysis
        metrics.update(self._evaluate_transition_recovery())
        
        # History assignment accuracy
        metrics['history_assignment_accuracy'] = self._evaluate_history_assignment()
        
        # Information-theoretic measures
        metrics.update(self._compute_information_distances())
        
        return metrics
    
    def _evaluate_transition_recovery(self) -> Dict[str, float]:
        """Evaluate recovery of true state transitions."""
        # Compare discovered transitions with ground truth
        # Implementation depends on specific ground truth format
        return {
            'transition_recovery_rate': 0.0,    # Placeholder
            'spurious_transition_rate': 0.0,    # Placeholder
            'transition_probability_rmse': 0.0   # Placeholder
        }
    
    def evaluate_prediction_performance(self) -> Dict[str, float]:
        """
        Evaluate prediction performance against theoretical optima.
        
        Returns:
            {
                'cross_entropy_ratio': float,       # Achieved vs optimal cross-entropy
                'perplexity_ratio': float,           # Achieved vs optimal perplexity
                'compression_efficiency': float,     # Compression relative to optimal
                'sample_efficiency': float           # Samples needed vs theoretical
            }
        """
        optimal_predictions = self.ground_truth.get('optimal_predictions', {})
        
        if not optimal_predictions:
            return {'error': 'No optimal predictions available'}
        
        # Compute prediction metrics for best CSSR result
        best_result = self._get_best_cssr_result()
        
        metrics = {
            'cross_entropy_ratio': self._compute_cross_entropy_ratio(best_result, optimal_predictions),
            'perplexity_ratio': self._compute_perplexity_ratio(best_result, optimal_predictions),
            'compression_efficiency': self._compute_compression_efficiency(best_result, optimal_predictions),
            'sample_efficiency': self._estimate_sample_efficiency(best_result, optimal_predictions)
        }
        
        return metrics
```

### 5. Performance Analysis (`performance_analyzer.py`)

```python
class PerformanceAnalyzer:
    """Comprehensive performance analysis and baseline comparison."""
    
    def analyze_scaling_behavior(self, cssr_results: Dict, metadata: Dict) -> Dict[str, Any]:
        """
        Analyze how CSSR performance scales with dataset properties.
        
        Returns:
            {
                'parameter_sensitivity': {...},     # Sensitivity to L and α
                'complexity_scaling': {...},        # Performance vs machine complexity  
                'sample_size_scaling': {...},       # Performance vs dataset size
                'convergence_analysis': {...}       # Convergence behavior analysis
            }
        """
        analysis = {}
        
        # Parameter sensitivity analysis
        analysis['parameter_sensitivity'] = self._analyze_parameter_sensitivity(cssr_results)
        
        # Complexity scaling
        machine_complexities = metadata.get('complexity_metrics', {})
        analysis['complexity_scaling'] = self._analyze_complexity_scaling(cssr_results, machine_complexities)
        
        # Sample size effects
        sequence_stats = metadata.get('sequence_statistics', {})
        analysis['sample_size_scaling'] = self._analyze_sample_scaling(cssr_results, sequence_stats)
        
        # Convergence behavior
        analysis['convergence_analysis'] = self._analyze_convergence_behavior(cssr_results)
        
        return analysis
    
    def compare_against_baselines(self, cssr_results: Dict, metadata: Dict) -> Dict[str, Any]:
        """
        Compare CSSR performance against various baselines.
        
        Returns:
            {
                'vs_random_baseline': {...},        # vs random prediction
                'vs_empirical_baselines': {...},    # vs n-gram models
                'vs_theoretical_optimal': {...},    # vs known optimal performance
                'relative_performance': {...}       # Overall performance assessment
            }
        """
        baselines = metadata.get('comparative_baselines', {})
        best_cssr = self._get_best_cssr_result(cssr_results)
        
        comparison = {}
        
        # Random baseline comparison
        if 'random_baselines' in baselines:
            comparison['vs_random_baseline'] = self._compare_vs_random(best_cssr, baselines['random_baselines'])
        
        # Empirical baselines comparison  
        if 'empirical_baselines' in baselines:
            comparison['vs_empirical_baselines'] = self._compare_vs_empirical(best_cssr, baselines['empirical_baselines'])
        
        # Theoretical optimal comparison
        if 'optimal_baselines' in baselines:
            comparison['vs_theoretical_optimal'] = self._compare_vs_optimal(best_cssr, baselines['optimal_baselines'])
        
        # Overall relative performance
        comparison['relative_performance'] = self._compute_relative_performance(comparison)
        
        return comparison
```

### 6. Results Visualization (`results_visualizer.py`)

```python
class ResultsVisualizer:
    """Generate comprehensive visualizations of classical CSSR analysis."""
    
    def generate_comprehensive_report(self, analysis_results: Dict, output_dir: str) -> str:
        """
        Generate HTML report with all analysis results and visualizations.
        
        Returns:
            Path to generated HTML report
        """
        # Create visualizations
        figures = {
            'parameter_heatmap': self._create_parameter_heatmap(analysis_results),
            'structure_comparison': self._create_structure_comparison(analysis_results),
            'performance_metrics': self._create_performance_dashboard(analysis_results),
            'scaling_analysis': self._create_scaling_plots(analysis_results),
            'baseline_comparison': self._create_baseline_comparison(analysis_results)
        }
        
        # Generate HTML report
        report_html = self._create_html_report(analysis_results, figures)
        
        # Save report
        report_path = Path(output_dir) / 'classical_cssr_analysis_report.html'
        with open(report_path, 'w') as f:
            f.write(report_html)
        
        return str(report_path)
    
    def _create_parameter_heatmap(self, results: Dict) -> str:
        """Create heatmap showing performance across parameter combinations."""
        # Implementation using matplotlib/plotly
        return "parameter_heatmap.png"
    
    def _create_structure_comparison(self, results: Dict) -> str:
        """Visualize discovered vs true causal structures."""
        # Implementation for structure visualization
        return "structure_comparison.png"
```

## Usage Examples

### Basic Analysis
```python
# Analyze single dataset
analyzer = ClassicalCSSRAnalyzer(
    dataset_dir="datasets/small_exp",
    output_dir="results/classical_analysis"
)

results = analyzer.run_complete_analysis(
    max_length=10,
    significance_level=0.05
)

print(f"Structure recovery accuracy: {results['evaluation']['structure_recovery']['state_count_accuracy']:.3f}")
print(f"Cross-entropy ratio: {results['performance']['cross_entropy_ratio']:.3f}")
```

### Batch Analysis Across Datasets
```python
# Compare classical CSSR across multiple datasets
datasets = ['small_exp', 'medium_exp', 'biased_exp']
comparative_results = {}

for dataset in datasets:
    analyzer = ClassicalCSSRAnalyzer(
        dataset_dir=f"datasets/{dataset}",
        output_dir=f"results/classical_analysis/{dataset}"
    )
    
    results = analyzer.run_complete_analysis()
    comparative_results[dataset] = results

# Generate comparative report
comparative_analyzer = ComparativeAnalyzer(comparative_results)
comparative_analyzer.generate_comparison_report("results/comparative_classical_analysis.html")
```

### Parameter Optimization
```python
# Find optimal CSSR parameters for dataset
optimizer = CSSRParameterOptimizer(
    dataset_dir="datasets/medium_exp",
    metric='structure_recovery_score'
)

optimal_params = optimizer.optimize_parameters(
    max_length_range=[6, 8, 10, 12, 15],
    significance_range=[0.001, 0.005, 0.01, 0.05, 0.1]
)

print(f"Optimal parameters: L={optimal_params['max_length']}, α={optimal_params['significance_level']}")
```

## Expected Outputs

### Analysis Results Structure
```json
{
  "dataset_info": {
    "experiment_name": "small_exp",
    "total_sequences": 2500,
    "sequence_length_range": [50, 200],
    "machines_used": 5,
    "quality_score": 0.987
  },
  "cssr_results": {
    "best_parameters": {"max_length": 10, "significance_level": 0.05},
    "discovered_states": 12,
    "convergence_achieved": true,
    "computation_time": 3.45
  },
  "evaluation_metrics": {
    "structure_recovery": {
      "state_count_accuracy": 0.92,
      "transition_recovery_rate": 0.89,
      "history_assignment_accuracy": 0.94
    },
    "prediction_performance": {
      "cross_entropy_ratio": 1.08,
      "perplexity_ratio": 1.12,
      "compression_efficiency": 0.95
    }
  },
  "baseline_comparison": {
    "vs_random": {"improvement_factor": 2.3},
    "vs_bigram": {"improvement_factor": 1.4},
    "vs_theoretical_optimal": {"efficiency": 0.87}
  }
}
```

### Generated Reports
- **HTML Analysis Report**: Comprehensive interactive report with all metrics and visualizations
- **Parameter Optimization Report**: Best parameter combinations for each dataset
- **Comparative Analysis**: Performance comparison across datasets and machine types
- **Baseline Establishment**: Classical CSSR baseline metrics for neural comparison

This framework provides the foundation for rigorous classical CSSR analysis that leverages all the rich metadata from your unified dataset generation system.