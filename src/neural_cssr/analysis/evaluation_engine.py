"""
Ground Truth Evaluation Engine for Classical CSSR Analysis

Comprehensive evaluation of CSSR results against known ground truth
with structure recovery and prediction performance metrics.
"""

from typing import Dict, List, Any, Optional, Tuple, Set
import numpy as np
from collections import defaultdict, Counter
import json


class GroundTruthEvaluator:
    """Comprehensive evaluation against known ground truth."""
    
    def __init__(self, cssr_results: Dict, ground_truth: Dict):
        """
        Initialize evaluator with CSSR results and ground truth.
        
        Args:
            cssr_results: Results from CSSR execution
            ground_truth: Ground truth information from dataset
        """
        self.cssr_results = cssr_results
        self.ground_truth = ground_truth
        
        # Extract best CSSR result for evaluation
        self.best_cssr_result = self._get_best_cssr_result()
        
    def evaluate_structure_recovery(self) -> Dict[str, float]:
        """
        Evaluate how well CSSR recovered the true causal structure.
        
        Returns:
            Dictionary with structure recovery metrics:
            {
                'state_count_accuracy': float,         # |discovered| vs |true|
                'transition_recovery_rate': float,     # Fraction of true transitions found
                'spurious_transition_rate': float,     # Fraction of false transitions
                'history_assignment_accuracy': float,  # Correct historyâ†’state mapping
                'information_distance': float,         # Information-theoretic distance
                'structural_similarity': float         # Overall structural similarity
            }
        """
        if not self.best_cssr_result:
            return {'error': 'No valid CSSR results to evaluate'}
        
        metrics = {}
        
        # State count comparison
        metrics['state_count_accuracy'] = self._evaluate_state_count_accuracy()
        
        # Transition recovery analysis
        transition_metrics = self._evaluate_transition_recovery()
        metrics.update(transition_metrics)
        
        # History assignment accuracy
        metrics['history_assignment_accuracy'] = self._evaluate_history_assignment()
        
        # Information-theoretic measures
        info_metrics = self._compute_information_distances()
        metrics.update(info_metrics)
        
        # Overall structural similarity
        metrics['structural_similarity'] = self._compute_structural_similarity(metrics)
        
        return metrics
    
    def evaluate_prediction_performance(self) -> Dict[str, float]:
        """
        Evaluate prediction performance against theoretical optima.
        
        Returns:
            Dictionary with prediction performance metrics:
            {
                'cross_entropy_ratio': float,       # Achieved vs optimal cross-entropy
                'perplexity_ratio': float,           # Achieved vs optimal perplexity
                'compression_efficiency': float,     # Compression relative to optimal
                'sample_efficiency': float,          # Samples needed vs theoretical
                'prediction_accuracy': float         # Overall prediction accuracy
            }
        """
        if not self.best_cssr_result:
            return {'error': 'No valid CSSR results to evaluate'}
        
        optimal_predictions = self.ground_truth.get('optimal_predictions', {})
        
        if not optimal_predictions:
            # Compute from ground truth if not available
            optimal_predictions = self._compute_theoretical_optimal()
        
        metrics = {
            'cross_entropy_ratio': self._compute_cross_entropy_ratio(optimal_predictions),
            'perplexity_ratio': self._compute_perplexity_ratio(optimal_predictions),
            'compression_efficiency': self._compute_compression_efficiency(optimal_predictions),
            'sample_efficiency': self._estimate_sample_efficiency(optimal_predictions),
            'prediction_accuracy': self._compute_prediction_accuracy()
        }
        
        return metrics
    
    def _get_best_cssr_result(self) -> Optional[Dict]:
        """Extract the best CSSR result from the results."""
        
        if 'parameter_results' in self.cssr_results:
            # Parameter sweep results
            best_params = self.cssr_results.get('best_parameters', {})
            overall_best = best_params.get('overall_best', {})
            param_key = overall_best.get('parameter_key')
            
            if param_key and param_key in self.cssr_results['parameter_results']:
                return self.cssr_results['parameter_results'][param_key]
            
            # Fallback: find first converged result
            for result in self.cssr_results['parameter_results'].values():
                if result.get('execution_info', {}).get('converged', False):
                    return result
            
            # Last resort: return any valid result
            valid_results = [r for r in self.cssr_results['parameter_results'].values() 
                           if 'error' not in r]
            if valid_results:
                return valid_results[0]
        
        elif 'discovered_structure' in self.cssr_results:
            # Single result
            return self.cssr_results
        
        return None
    
    def _evaluate_state_count_accuracy(self) -> float:
        """Evaluate accuracy of discovered state count."""
        
        true_machines = self.ground_truth.get('machine_definitions', {})
        if not true_machines:
            return 0.0
        
        # Get true state counts
        true_state_counts = []
        for machine_def in true_machines.values():
            if isinstance(machine_def, dict) and 'states' in machine_def:
                true_state_counts.append(len(machine_def['states']))
        
        if not true_state_counts:
            return 0.0
        
        # Average true state count (for multi-machine datasets)
        avg_true_states = np.mean(true_state_counts)
        
        # Discovered state count
        discovered_states = self.best_cssr_result.get('discovered_structure', {}).get('num_states', 0)
        
        if avg_true_states == 0:
            return 1.0 if discovered_states == 0 else 0.0
        
        # Compute accuracy as 1 - relative error
        relative_error = abs(discovered_states - avg_true_states) / avg_true_states
        accuracy = max(0.0, 1.0 - relative_error)
        
        return accuracy
    
    def _evaluate_transition_recovery(self) -> Dict[str, float]:
        """Evaluate recovery of true state transitions."""
        
        # This is a simplified implementation
        # In practice, you'd need to align discovered states with true states
        
        true_machines = self.ground_truth.get('machine_definitions', {})
        if not true_machines:
            return {
                'transition_recovery_rate': 0.0,
                'spurious_transition_rate': 0.0,
                'transition_probability_rmse': 0.0
            }
        
        # Count true transitions across all machines
        total_true_transitions = 0
        for machine_def in true_machines.values():
            if isinstance(machine_def, dict) and 'transitions' in machine_def:
                transitions = machine_def['transitions']
                if isinstance(transitions, dict):
                    total_true_transitions += len(transitions)
        
        # Count discovered transitions
        discovered_structure = self.best_cssr_result.get('discovered_structure', {})
        discovered_transitions = discovered_structure.get('transitions', {})
        
        total_discovered_transitions = 0
        if isinstance(discovered_transitions, dict):
            for state_transitions in discovered_transitions.get('state_transitions', {}).values():
                if isinstance(state_transitions, dict):
                    total_discovered_transitions += len(state_transitions)
        
        # Compute recovery metrics (simplified)
        if total_true_transitions == 0:
            recovery_rate = 1.0 if total_discovered_transitions == 0 else 0.0
            spurious_rate = 0.0
        else:
            # This is a rough approximation - proper evaluation would require state alignment
            recovery_rate = min(1.0, total_discovered_transitions / total_true_transitions)
            spurious_rate = max(0.0, (total_discovered_transitions - total_true_transitions) / total_true_transitions)
        
        return {
            'transition_recovery_rate': recovery_rate,
            'spurious_transition_rate': spurious_rate,
            'transition_probability_rmse': 0.0  # Would need state alignment to compute
        }
    
    def _evaluate_history_assignment(self) -> float:
        """Evaluate accuracy of history-to-state assignments."""
        
        # This would require detailed comparison of how histories are assigned to states
        # For now, return a placeholder based on structural similarity
        
        causal_state_labels = self.ground_truth.get('causal_state_labels', {})
        if not causal_state_labels:
            return 0.0
        
        # Simplified evaluation based on number of states
        state_count_accuracy = self._evaluate_state_count_accuracy()
        
        # History assignment is typically correlated with state count accuracy
        # but would be lower due to the difficulty of exact alignment
        return state_count_accuracy * 0.8  # Rough approximation
    
    def _compute_information_distances(self) -> Dict[str, float]:
        """Compute information-theoretic distances between discovered and true structures."""
        
        # Compute entropy of discovered structure
        discovered_entropy = self._compute_discovered_entropy()
        
        # Get true entropy from metadata
        true_entropy = self._get_true_entropy()
        
        # Compute information distance
        if true_entropy > 0:
            entropy_ratio = discovered_entropy / true_entropy
            information_distance = abs(1.0 - entropy_ratio)
        else:
            information_distance = 1.0 if discovered_entropy > 0 else 0.0
        
        return {
            'information_distance': information_distance,
            'entropy_ratio': entropy_ratio if true_entropy > 0 else 0.0,
            'discovered_entropy': discovered_entropy,
            'true_entropy': true_entropy
        }
    
    def _compute_discovered_entropy(self) -> float:
        """Compute entropy of discovered structure."""
        
        discovered_structure = self.best_cssr_result.get('discovered_structure', {})
        states = discovered_structure.get('states', {})
        
        if not states:
            return 0.0
        
        total_entropy = 0.0
        total_weight = 0.0
        
        for state_info in states.values():
            if isinstance(state_info, dict):
                entropy = state_info.get('entropy', 0.0)
                weight = state_info.get('total_observations', 1.0)
                
                total_entropy += entropy * weight
                total_weight += weight
        
        if total_weight > 0:
            return total_entropy / total_weight
        else:
            return 0.0
    
    def _get_true_entropy(self) -> float:
        """Get true entropy from ground truth or statistical metadata."""
        
        # Try to get from statistical analysis
        stats = self.ground_truth.get('information_measures', {})
        if stats and 'entropy' in stats:
            return stats['entropy']
        
        # Try to compute from machine definitions
        true_machines = self.ground_truth.get('machine_definitions', {})
        if true_machines:
            # This would require computing entropy from machine transition probabilities
            # For now, return a placeholder
            return 1.0  # Placeholder
        
        return 0.0
    
    def _compute_structural_similarity(self, metrics: Dict[str, float]) -> float:
        """Compute overall structural similarity score."""
        
        # Combine multiple metrics into overall similarity score
        weights = {
            'state_count_accuracy': 0.3,
            'transition_recovery_rate': 0.3,
            'history_assignment_accuracy': 0.2,
            'information_distance': 0.2  # Invert this since lower is better
        }
        
        similarity = 0.0
        total_weight = 0.0
        
        for metric, weight in weights.items():
            if metric in metrics:
                value = metrics[metric]
                
                # Invert information distance since lower is better
                if metric == 'information_distance':
                    value = 1.0 - min(1.0, value)
                
                similarity += value * weight
                total_weight += weight
        
        if total_weight > 0:
            return similarity / total_weight
        else:
            return 0.0
    
    def _compute_theoretical_optimal(self) -> Dict[str, Any]:
        """Compute theoretical optimal predictions from ground truth."""
        
        # This would compute the theoretical optimal performance
        # based on the true machine structure
        
        return {
            'optimal_cross_entropy': 1.0,  # Placeholder
            'optimal_perplexity': 2.0,     # Placeholder
            'optimal_compression': 0.5     # Placeholder
        }
    
    def _compute_cross_entropy_ratio(self, optimal_predictions: Dict) -> float:
        """Compute cross-entropy ratio (achieved/optimal)."""
        
        # Get achieved cross-entropy from CSSR result
        prediction_perf = self.best_cssr_result.get('prediction_performance', {})
        achieved_entropy = prediction_perf.get('avg_state_entropy', 0.0)
        
        # Get optimal cross-entropy
        optimal_entropy = optimal_predictions.get('optimal_cross_entropy', 1.0)
        
        if optimal_entropy > 0:
            return achieved_entropy / optimal_entropy
        else:
            return 1.0 if achieved_entropy == 0 else float('inf')
    
    def _compute_perplexity_ratio(self, optimal_predictions: Dict) -> float:
        """Compute perplexity ratio (achieved/optimal)."""
        
        # Perplexity = 2^entropy
        prediction_perf = self.best_cssr_result.get('prediction_performance', {})
        achieved_entropy = prediction_perf.get('avg_state_entropy', 0.0)
        achieved_perplexity = 2 ** achieved_entropy
        
        optimal_perplexity = optimal_predictions.get('optimal_perplexity', 2.0)
        
        if optimal_perplexity > 0:
            return achieved_perplexity / optimal_perplexity
        else:
            return 1.0 if achieved_perplexity == 0 else float('inf')
    
    def _compute_compression_efficiency(self, optimal_predictions: Dict) -> float:
        """Compute compression efficiency relative to optimal."""
        
        # This would compare compression achieved by CSSR vs theoretical optimal
        # For now, return a placeholder based on entropy
        
        prediction_perf = self.best_cssr_result.get('prediction_performance', {})
        achieved_entropy = prediction_perf.get('avg_state_entropy', 0.0)
        
        # Lower entropy = better compression
        if achieved_entropy > 0:
            return min(1.0, 1.0 / achieved_entropy)
        else:
            return 1.0
    
    def _estimate_sample_efficiency(self, optimal_predictions: Dict) -> float:
        """Estimate sample efficiency compared to theoretical optimum."""
        
        # This would estimate how many samples CSSR needed vs theoretical minimum
        # For now, return a placeholder based on convergence
        
        execution_info = self.best_cssr_result.get('execution_info', {})
        converged = execution_info.get('converged', False)
        
        # Simple heuristic: converged results are more sample efficient
        return 0.8 if converged else 0.5
    
    def _compute_prediction_accuracy(self) -> float:
        """Compute overall prediction accuracy."""
        
        # This would evaluate actual prediction accuracy on held-out data
        # For now, return a placeholder based on structural similarity
        
        structure_metrics = self.evaluate_structure_recovery()
        structural_similarity = structure_metrics.get('structural_similarity', 0.0)
        
        # Prediction accuracy is typically correlated with but lower than structural similarity
        return structural_similarity * 0.9


class ComparativeEvaluator:
    """Compare CSSR performance across multiple datasets or parameter settings."""
    
    def __init__(self, evaluation_results: Dict[str, Dict]):
        """
        Initialize with evaluation results from multiple analyses.
        
        Args:
            evaluation_results: Dict mapping dataset/config names to evaluation results
        """
        self.evaluation_results = evaluation_results
    
    def compare_structure_recovery(self) -> Dict[str, Any]:
        """Compare structure recovery across datasets."""
        
        comparison = {
            'dataset_scores': {},
            'aggregate_metrics': {},
            'rankings': {}
        }
        
        # Extract structure recovery scores
        scores = {}
        for dataset, results in self.evaluation_results.items():
            structure_recovery = results.get('evaluation_metrics', {}).get('structure_recovery', {})
            if structure_recovery and 'structural_similarity' in structure_recovery:
                scores[dataset] = structure_recovery['structural_similarity']
        
        if not scores:
            return {'error': 'No structure recovery scores to compare'}
        
        comparison['dataset_scores'] = scores
        
        # Compute aggregate metrics
        score_values = list(scores.values())
        comparison['aggregate_metrics'] = {
            'mean_score': np.mean(score_values),
            'std_score': np.std(score_values),
            'min_score': min(score_values),
            'max_score': max(score_values),
            'range': max(score_values) - min(score_values)
        }
        
        # Create rankings
        sorted_datasets = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        comparison['rankings'] = {
            'best_to_worst': [dataset for dataset, _ in sorted_datasets],
            'scores_ordered': [score for _, score in sorted_datasets]
        }
        
        return comparison
    
    def compare_prediction_performance(self) -> Dict[str, Any]:
        """Compare prediction performance across datasets."""
        
        comparison = {
            'cross_entropy_ratios': {},
            'perplexity_ratios': {},
            'aggregate_metrics': {},
            'rankings': {}
        }
        
        # Extract prediction metrics
        cross_entropy_ratios = {}
        perplexity_ratios = {}
        
        for dataset, results in self.evaluation_results.items():
            pred_perf = results.get('evaluation_metrics', {}).get('prediction_performance', {})
            
            if 'cross_entropy_ratio' in pred_perf:
                cross_entropy_ratios[dataset] = pred_perf['cross_entropy_ratio']
            
            if 'perplexity_ratio' in pred_perf:
                perplexity_ratios[dataset] = pred_perf['perplexity_ratio']
        
        comparison['cross_entropy_ratios'] = cross_entropy_ratios
        comparison['perplexity_ratios'] = perplexity_ratios
        
        # Compute aggregate metrics
        if cross_entropy_ratios:
            ce_values = list(cross_entropy_ratios.values())
            comparison['aggregate_metrics']['cross_entropy'] = {
                'mean': np.mean(ce_values),
                'std': np.std(ce_values),
                'min': min(ce_values),
                'max': max(ce_values)
            }
            
            # Rankings (lower is better for cross-entropy ratio)
            sorted_ce = sorted(cross_entropy_ratios.items(), key=lambda x: x[1])
            comparison['rankings']['cross_entropy_best_to_worst'] = [d for d, _ in sorted_ce]
        
        return comparison
    
    def generate_comparative_report(self) -> Dict[str, Any]:
        """Generate comprehensive comparative report."""
        
        report = {
            'summary': {
                'total_datasets': len(self.evaluation_results),
                'datasets_analyzed': list(self.evaluation_results.keys())
            },
            'structure_recovery_comparison': self.compare_structure_recovery(),
            'prediction_performance_comparison': self.compare_prediction_performance(),
            'overall_rankings': self._compute_overall_rankings()
        }
        
        return report
    
    def _compute_overall_rankings(self) -> Dict[str, Any]:
        """Compute overall rankings combining multiple metrics."""
        
        # Combine structure recovery and prediction performance
        overall_scores = {}
        
        for dataset, results in self.evaluation_results.items():
            eval_metrics = results.get('evaluation_metrics', {})
            
            # Structure recovery score
            structure_score = eval_metrics.get('structure_recovery', {}).get('structural_similarity', 0.0)
            
            # Prediction performance score (invert cross-entropy ratio since lower is better)
            pred_perf = eval_metrics.get('prediction_performance', {})
            ce_ratio = pred_perf.get('cross_entropy_ratio', 1.0)
            pred_score = 1.0 / max(ce_ratio, 0.1)  # Invert and avoid division by zero
            
            # Combined score (equal weighting)
            overall_scores[dataset] = (structure_score + pred_score) / 2.0
        
        if not overall_scores:
            return {'error': 'No scores available for ranking'}
        
        # Create rankings
        sorted_overall = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)
        
        return {
            'overall_scores': overall_scores,
            'ranking': [dataset for dataset, _ in sorted_overall],
            'best_dataset': sorted_overall[0][0] if sorted_overall else None,
            'worst_dataset': sorted_overall[-1][0] if sorted_overall else None
        }